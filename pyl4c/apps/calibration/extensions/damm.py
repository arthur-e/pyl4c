'''
Calibration of the DAMM model based on inverted flux tower data, which
is used to infer empirical C substrate storage. This approach fits modeled
(heterotrophic) respiration (RH) to tower RH, whereas L4C fit modeled
RECO to tower RECO. This is made possible by using the carbon-use efficiency
parameters from L4C to calculate NPP. However, an alternative would be to
fit carbon-use efficiency directly, as L4C does. The `moisture_ptile` or
`temp_ptile` and `cbar_ptile` options are similar to the `kmult_perc` and
`quoperc` percentile choices in the L4C calibration (Matlab) code.

In determining boundary conditions, we considered the following, after
Davidson et al. (2012, p.375), using the median D_gas to compute km_O2:

- When soil is saturated: [S_x] = [S_{soluble}] --> D_liq = (1 / VWC^3)
    np.where(np.divide(soil_m, porosity * 100) > 0.95, 1 / np.power(soil_m / 100, 3), np.nan)
- When soil is completely dry: [O2] = 0.2095 --> D_gas = 1 / a^(4/3)
    np.where(soil_m < 5, 1 / np.power(porosity - (soil_m / 100), 4/3), np.nan)
- For half-saturation constant km_O2, under mean soil moisture conditions
    (here, 23.5% VWC), km_O2 should equal [O2]: km_O2 = D_gas * 0.209 * a^(4/3)
    (3.78 * 0.209 * np.power(porosity - (soil_m.mean(axis = 0) / 100), 4/3))

Example uses:

    python damm.py run --pft=1 --fit-cue=True
    python damm.py run --pft=1 --trials=0
    python damm.py run --pft=1 --moisture-ptile0=20 --cbar-ptile-75 --window-size=3
    python damm.py run --pft=1 --CUE=0.6 # Take a guess at parameter values
'''

import csv
import datetime
import os
import pickle
import h5py
import matplotlib
import nlopt
import numpy as np
import warnings
from functools import partial
from scipy.stats import gmean
from scipy.signal import filtfilt
from matplotlib import pyplot
from pyl4c import pft_dominant, suppress_warnings
from pyl4c.data.fixtures import BPLUT
from pyl4c.stats import rmsd
from pyl4c.apps.l4c.extensions.damm import DAMMDecompositionModel, DAMMDecompositionModel2, g_cm3_to_g_m2, g_m2_to_g_cm3
from pyl4c.apps.calibration import GenericOptimization, report_fit_stats, solve_least_squares

# NOTE: This file was generated by pyl4c.apps.calibration.legacy
# INPUTS_HDF = '/anx_lagr3/arthur.endsley/SMAP_L4C/calibration/v5_Y2020/L4_C_tower_site_drivers_NRv7-2_for_356_sites.h5'
INPUTS_HDF = '/home/arthur/Downloads/L4C_experiments/L4_C_tower_site_drivers_NRv7-2_for_356_sites.h5'
TOWER_HDF = '/anx_lagr3/arthur.endsley/SMAP_L4C/calibration/Fluxnet2015_LaThuile_tower_data_for_356_sites.h5'
EXCLUDE_PATH = '/home/arthur/Workspace/NTSG/SMAP_L4C/calibration/FluxTower_sites_Vv4040_excluded.csv'
STORAGE_PATH = '/home/arthur/Workspace/NTSG/SMAP_L4C/applications/DAMM/DAMM-NLOPT_calibration_scratch_20201116.pickle'
SOIL_DEPTH_CM = 5
matplotlib.use('TkAgg')
pyplot.rcParams.update({'font.size': 12})

class CLI(object):
    def __init__(
            self, search = 'global', method = 'nlopt',
            model = DAMMDecompositionModel2, trials = 1, fit_cue = True,
            fix = None, moisture_ptile0 = 5, moisture_ptile1 = 95,
            temp_ptile = 5, cbar_ptile = 75, smooth = True, window_size = 2,
            plots = True, minimize_rmse = True, exclude = True,
            use_legacy_pft = True, use_l4c_soc = True):
        '''
        Parameters
        ----------
        search : str
            Type of parameter sweep to execute, one of: "global" (Default)
            or "local"
        method : str
            Optimization framework to use, one of: "nlopt" (Default) or "scipy"
        model : AbstractDAMM
            The version of the DAMM model to use, indicated by its class
        trials : int
            The number of randomized trials (randomized initial parameter values).
            If trials=0, the model will NOT be fit and instead goodness-of-fit
            metrics for the last (saved) fit will be shown.
        fit_cue : bool
            True to fit carbon use efficiency (CUE) (Default: True)
        moisture_ptile0 : int
            Cutoff, as a percentile, for low soil moisture conditions
            (Default: 5, e.g., lower 5% observations by soil moisture are
            discarded
        moisture_ptile1 : int
            Cutoff, as a percentile, for high soil moisture conditions
            (Default: 95, e.g., upper 5% observations by soil moisture are
            discarded
        temp_ptile : int
            Cutoff, as a percentile, for soil temperature conditions (Default: 5);
            e.g., lower 5% observations by soil temperature are discarded
        cbar_ptile : int
            Percentile of site-level Cbar to use for substrate input to
            respiration
        smooth : bool
            True to apply a low-pass filter to the flux tower time series data
            (Default: True)
        window_size : int
            Length (in station time steps, days) of the filter window (Default: 2)
        plots : bool
            True to display goodness-of-fit plots (Default: True)
        minimize_rmse : bool
            True to use minimum RMSE as score for multiple trials; otherwise, uses
            maximum R-squared (Default: True)
        exclude : bool
            True to exclude some sites, according to the L4C calibration
            protocol; sites are chosen in advance (Default: True)
        use_legacy_pft : bool
            True to use the L4C Nature Run v7.2 "legacy" PFT map ("lc_dom")
            for the PFT class assignments of each pixel (Default: True)
        use_l4c_soc : bool
            True to use the SOC in each pool from the L4C forward run driver
            dataset as the initial substrate (Default: True)
        '''
        np.seterr(invalid = 'ignore')
        assert search in ('global', 'local'),\
            '"search" can either be "global" (default) or "local"'
        assert method in ('nlopt', 'scipy'),\
            '"method" can either be "nlopt" (default) or "scipy"'
        assert method == 'local' and trials <= 1 or (method != 'local'),\
            'Method "local" cannot be used with random trials (set trials <= 1)'
        if cbar_ptile < 1 or moisture_ptile0 < 1 or temp_ptile < 1:
            print('WARNING: Percentiles should be specified on the domain [0, 100] not [0, 1]')
        self._search = search
        self._method = method
        self._model = model
        self._trials = trials
        self._fit_cue = fit_cue
        self._moisture_ptile0 = moisture_ptile0
        self._moisture_ptile1 = moisture_ptile1
        self._temp_ptile = temp_ptile
        self._cbar_ptile = cbar_ptile
        self._smooth = smooth
        self._window_size = window_size
        self._plots = plots
        self._minimize_rmse = minimize_rmse
        self._exclude = exclude
        self._use_legacy_pft = use_legacy_pft
        self._use_l4c_soc = use_l4c_soc

    @suppress_warnings
    def _load_data(self, pft):
        '''
        Parameters
        ----------
        pft : int
            The PFT class to calibrate
        '''
        excluded = []
        if self._exclude:
            with open(EXCLUDE_PATH, 'r') as file:
                reader = csv.reader(file)
                for _, site_id in reader:
                    excluded.append(site_id)

        soc = None
        with h5py.File(INPUTS_HDF, 'r') as hdf:
            site_ids = hdf['site_id'][:]
            # Find duplicate (shared) 9-km cells, calculate site weights
            idx = hdf['coords/grid_9km_idx'][:].round(0).astype(np.int16)
            uid = ['%d%d' % (r, c) for r, c in idx.tolist()]
            weights = np.array([(1.0 / uid.count(x)) for x in uid])
            # Set weight of excluded sites to zero
            if self._exclude:
                weights = np.where(np.in1d(site_ids, excluded), 0, weights)
            # Then, read in the data for the subset of sites we're calibrating on
            selector = self.sites(pft, excluded, self._use_legacy_pft)
            weights = weights[selector]
            porosity = hdf['state/porosity'][selector]
            # NOTE: Converting from "wetness" to volumetric water content (VWC)
            #   (in % units); this requires multiplying (wetness * porosity) as
            #   (wetness = VWC / porosity)
            soil_m = 100 * np.multiply( # Convert from and back to % units
                hdf['drivers/smsf'][:,selector] / 100, porosity)
            soil_t = hdf['drivers/tsoil'][:,selector]
            # Average litterfall per day
            litterfall = (hdf['state/npp_sum'][selector,:].sum(axis = 1) / 365)
            # Convert from g m-2 to g m-3, then to g cm-3
            litterfall = (litterfall / (SOIL_DEPTH_CM / 100)) / 1e6
            # Read APAR for masking GPP
            if self._use_l4c_soc:
                soc = hdf['state/soil_organic_carbon'][:,selector,:].sum(axis = 2)
            apar = np.multiply(
                hdf['drivers/fpar'][:,selector,:].mean(axis = 2),
                hdf['drivers/par'][:,selector])

        # Load tower data, mask out GPP < 0 and when APAR < 0.1
        with h5py.File(TOWER_HDF, 'r') as hdf:
            assert np.equal(site_ids, hdf['site_id'][:].tolist()).all(),\
                'Drivers file and tower fluxes file do not have site IDs in the same order!'
            reco_tower = hdf['RECO'][:,selector]
            reco_tower = np.where(reco_tower < 0, np.nan, reco_tower)
            gpp_tower = hdf['GPP'][:,selector]
            gpp_tower = np.where(
                np.logical_or(gpp_tower < 0, apar < 0.1), np.nan, gpp_tower)
            apar = None

        # Filter the tower data
        if self._smooth:
            reco_tower = smooth_series(reco_tower, self._window_size)
            gpp_tower = smooth_series(gpp_tower, self._window_size)

        self._data = {
            'gpp_tower': gpp_tower,
            'reco_tower': reco_tower,
            'soil_m': soil_m,
            'soil_t': soil_t,
            'porosity': porosity,
            'litterfall': litterfall,
            'weights': weights,
            'site_ids': site_ids
        }
        if self._use_l4c_soc:
            self._data['soc'] = g_m2_to_g_cm3(soc)
        if not self._fit_cue:
            self._cue = BPLUT['CUE'][pft]

    def _configure(self, pft):
        '''
        Sets the initial parameter values, the bounds, and the step size for
        optimization.

        Parameters
        ----------
        pft : int
            The PFT class to calibrate

        Returns
        -------
        tuple
            (init_params, bounds, step_size, labels)
        '''
        # IMPORTANT: Get the parameter labels
        labels = list(self._model.parameter_names)
        # After Davidson et al. (2012), (1, 10, 50, 90, 99th) percentiles
        #   and using <5% and >95% VWC for "completely" dry, saturated conditions:
        #   d_liq: [  2.2,   6.5,   7.2,  15.7, 17.8 ]
        #   d_gas: [3.109, 3.508, 3.781, 4.088, 4.341]
        #   km_O2: [0.024, 0.053, 0.1  , 0.161, 0.247], mean = 0.105;
        #       however, if VWC = 0.235 is fixed, mean = 0.107
        if self._model == DAMMDecompositionModel:
            bounds = [ # alpha0, alpha1, alpha2, ea, km_s, p, d_liq, d_gas
                np.array(( 10,  1, 0.001, 70,    0,   0,  2,  3)),
                np.array((1e2, 10,   0.1, 78, 0.01, 0.1, 20, 10))
            ]
        else:
            bounds = [ # alpha0, alpha1, alpha2, ea, km_s_int, km_s_slope, p, d_liq, d_gas, km_O2
                np.array(( 10,  1, 0.001, 70,-0.01,-0.0001, 2e-2,  2, 3, 0.05)),
                np.array((1e2, 10,   0.1, 78, 0.01, 0.0001,  0.1, 20, 5, 0.15))
            ]
        # If we want to do a global search, forget what we already know about
        #   this problem and take larger step sizes
        if self._search == 'global':
            if self._model == DAMMDecompositionModel:
                step_size = ( 0.1, 0.01, 0.001, 0.2, 1e-5, 1e-3, 0.1, 0.1)
            else:
                step_size = ( 0.1, 0.01, 0.001, 0.2, 1e-5, 1e-8, 1e-3, 0.1, 0.1,  0.01)
        elif self._search == 'local':
            if self._model == DAMMDecompositionModel:
                step_size = (0.01, 0.001, 1e-4, 0.1, 1e-7, 1e-5, 0.01, 0.01)
            else:
                step_size = (0.01, 0.001, 1e-4, 0.1, 1e-7, 1e-5, 1e-10, 0.01, 0.01, 0.001)
            try:
                with open(STORAGE_PATH, 'rb') as file:
                    data_dict = pickle.load(file)
                    results = data_dict['parameters']
                init_params = results[pft-1,:]
                assert not np.all(np.isnan(init_params))
            except:
                raise ValueError('Cannot perform local search until a global search has been done')
        # Concatenate CUE fit parameters
        if self._fit_cue:
            # Collalti & Prentice (2019) mean of 0.47 +/- 2,1 sigma
            bounds[0] = np.hstack((bounds[0], 0.35))
            bounds[1] = np.hstack((bounds[1], 0.71))
            step_size = tuple([*step_size, 0.005])
            labels.append('CUE')
        if self._search == 'global':
            init_params = [np.nan] * len(labels)
        return (init_params, bounds, step_size, labels)

    def _plot_results(
            self, obs, pred, start_date = datetime.datetime(2000, 1, 1)):
        '''
        Plot the best-fit site, longest-record site, and predicted-versus-observed
        scatterplot.

        Parameters
        ----------
        obs : numpy.ndarray
        pred : numpy.ndarray
        start_date : datetime.datetime
        '''
        timestamps = [
            start_date + datetime.timedelta(d) for d in range(0, obs.shape[0])
        ]
        weights, site_ids = [self._data[k] for k in ('weights', 'site_ids')]
        fits = np.array([
            rmsd(obs[:,i], pred[:,i], weights = weights[i])
            for i in range(0, obs.shape[1])
        ])
        fits[np.isnan(fits)] = np.inf
        # Ensure that records too short will not be picked
        fits[(~np.isnan(obs)).astype(np.int8).sum(axis = 0) < (2*365)] = np.inf
        # Get the index of the station with the best fit
        idx = np.argmin(fits)
        fig = pyplot.figure(figsize = (8, 6))
        pyplot.plot(timestamps, obs[:,idx], ',-', label = 'Observed RH')
        pyplot.plot(timestamps, pred[:,idx], 'r-', label = 'Predicted RH')
        pyplot.ylabel(r'RH $(g\,C\,m^{-2}\,d^{-1})$')
        pyplot.title('Best-fit station (%s)' % site_ids[idx])
        pyplot.legend()
        pyplot.show()
        # Get the index of the station with the longest record
        idx = np.argmin(np.isnan(obs).astype(np.int8).sum(axis = 0))
        pyplot.figure(figsize = (8, 6))
        pyplot.plot(timestamps, obs[:,idx], ',-', label = 'Observed RH')
        pyplot.plot(timestamps, pred[:,idx], 'r-', label = 'Predicted RH')
        pyplot.ylabel(r'RH $(g\,C\,m^{-2}\,d^{-1})$')
        pyplot.title('Longest-record station (%s)' % site_ids[idx])
        pyplot.legend()
        pyplot.show()
        fig, ax = pyplot.subplots(figsize = (5, 5))
        pyplot.plot(obs.ravel(), pred.ravel(), 'k.', alpha = 0.01)
        # Plot diagonal line
        pyplot.plot([0, 1], [0, 1], transform = ax.transAxes,
            linestyle = 'dashed', color = 'r')
        pyplot.xlim(0, np.nanmax(pred))
        pyplot.ylim(0, np.nanmax(pred))
        pyplot.xlabel(r'Observed RH $(g\,C\,m^{-2}\,d^{-1})$')
        pyplot.ylabel(r'Predicted RH $(g\,C\,m^{-2}\,d^{-1})$')
        pyplot.show()

    @suppress_warnings
    def _report_fit(self, init_params, fit_params, labels, plots = True):
        '''
        Compare observations and predictions and report fit statistics.

        Parameters
        ----------
        init_params : numpy.ndarray
        fit_params : numpy.ndarray
        labels : list or tuple
        plots : bool
            True to plot fit results (Default: True)

        Returns
        -------
        tuple
            Fit statistics: (R-squared, RMSE, ubRMSE, Bias)
        '''
        # Get carbon-use efficiency (used to segment fluxes)
        cue = fit_params[
            labels.index('CUE')
        ] if self._fit_cue else self._cue
        gpp_tower, reco_tower, soil_m, soil_t, porosity, litterfall, weights = [
            self._data[k] for k in list(self._data.keys())[0:7]
        ]
        damm = self._model(fit_params)
        # Calculate observed RH as RECO - RA
        obs = reco_tower - ((1 - cue) * gpp_tower)
        obs = np.where(obs < 0, np.nan, obs)
        # Filter out extreme conditions before calculating Cbar
        soil_m0 = np.where(np.logical_or(
            soil_m < np.nanpercentile(soil_m, self._moisture_ptile0, axis = 0),
            soil_m > np.nanpercentile(soil_m, self._moisture_ptile1, axis = 0)), np.nan, soil_m)
        soil_t0 = np.where(
            soil_t < np.nanpercentile(soil_t, self._temp_ptile, axis = 0), np.nan, soil_t)
        if self._use_l4c_soc:
            substrate = [self._data['soc'][i,...] for i in range(0, 3)]
        else:
            substrate = damm.cbar(
                obs, soil_m0, soil_t0, porosity, perc = self._cbar_ptile)
        # Get predicted respiration
        pred = damm.total_respiration(substrate, soil_m, soil_t, porosity)
        pred[np.isnan(obs)] = np.nan
        if not self._use_l4c_soc:
            # Print Cbar distribution
            print('1st, 10th, 50th, 90th, and 99th percentiles of Cbar:')
            for i in range(0, 3):
                print('-- ', np.nanpercentile(
                    g_cm3_to_g_m2(substrate[i]), (1, 10, 50, 90, 99)).round(2))
        # Print RH distribution
        print('1st, 10th, 50th, 90th, and 99th percentiles of RH:')
        print('-- ', np.nanpercentile(pred, (1, 10, 50, 90, 99)).round(3))
        # Report parameters
        self._report_params(labels, fit_params, init_params)
        # Report fit statistics
        stats = report_fit_stats(obs, pred, weights)
        if (plots and self._plots):
            self._plot_results(obs, pred)
        return stats

    def _report_params(self, labels, fit_params, init_params):
        'Prints the initial and updated parameter values'
        prec = 3
        pad = max(len(l) for l in labels) + 1
        fmt_string = '-- {:<%d} {:>%d} [{:>%d}]' % (pad, 8 + prec, 8 + prec)
        print((' {:>%d} {:>%d}' % (11 + pad + prec, 9 + prec))\
            .format('NEW', 'INITIAL'))
        for i, label in enumerate(labels):
            char = 'E' if fit_params[i] < 0.1 else 'f'
            new = ('%%.%d%s' % (prec, char)) % fit_params[i] if fit_params[i] is not None else ''
            if np.isnan(init_params[i]):
                old = ''
            else:
                old = ('%%.%d%s' % (prec, char)) % init_params[i]
            print(fmt_string.format(('%s:' % label), new, old))

    @suppress_warnings
    def _residuals(
            self, params, gpp_tower, reco_tower, soil_m, soil_t, porosity,
            litterfall, weights, site_ids, labels, soc = None, mlow = 5,
            mhigh = 95, tlow = 5, drop_nan = True):
        '''
        Prototype objective function: Difference between tower RH and DAMM RH
        or between tower RECO and DAMM RECO.

        Parameters
        ----------
        params : tuple or list or numpy.ndarray
            Current model parameters
        '''
        # Get carbon-use efficiency (used to segment fluxes)
        cue = params[
            labels.index('CUE')
        ] if self._fit_cue else self._cue
        damm = self._model(params)
        # Calculate tower RH as RECO - RA
        rh_tower = reco_tower - ((1 - cue) * gpp_tower)
        # Filter out extreme conditions before calculating Cbar
        soil_m0 = np.where(np.logical_or(
            soil_m < np.nanpercentile(soil_m, mlow, axis = 0),
            soil_m > np.nanpercentile(soil_m, mhigh, axis = 0)), np.nan, soil_m)
        soil_t0 = np.where(
            soil_t < np.nanpercentile(soil_t, tlow, axis = 0), np.nan, soil_t)
        # Get empirical C storage in each pool, calculate total RH flux;
        #   globals rh_tower, soil_m, soil_t, porosity
        if soc is None:
            cbar_i = damm.cbar(
                rh_tower, soil_m0, soil_t0, porosity, perc = self._cbar_ptile)
        else:
            cbar_i = soc.tolist()
        rh = damm.total_respiration(cbar_i, soil_m, soil_t, porosity)
        if self._fit_cue:
            reco = np.add(rh, ((1 - cue) * gpp_tower))
            diff = np.subtract(reco_tower, np.where(reco < 0, 0, reco))
        else:
            diff = np.subtract(rh_tower, rh)
        # Multiply by the tower weights
        if drop_nan:
            return (weights * diff)[~np.isnan(diff)]
        return (weights * diff)

    def plot_residuals(self, pft, driver, xlim = None, ylim = None, alpha = 0.01):
        '''
        Plots the model residuals for the given PFT against soil moisture
        or soil temperature.

        Parameters
        ----------
        pft : int
            The plant functional type (PFT) chosen
        driver : str
            Either soil "moisture" or "temperature"
        xlim : list
        ylim : list
        alpha : float
        '''
        assert driver in ('moisture', 'temperature'),\
            '"--driver" must be one of: "moisture", "temperature"'
        driver = 'soil_m' if driver == 'moisture' else 'soil_t'
        self._load_data(pft)
        with open(STORAGE_PATH, 'rb') as file:
            data_dict = pickle.load(file)
            params = data_dict['parameters']
            labels = data_dict['metadata']

        residuals = self._residuals(
            params[(pft-1),:], **self._data, labels = labels, drop_nan = False)
        pyplot.plot(self._data[driver].ravel(), residuals.ravel(), 'k+', alpha = alpha)
        if xlim is None:
            xlim = (np.nanmin(self._data[driver]), np.nanmax(self._data[driver]))
        pyplot.xlim(*xlim)
        if ylim is not None:
            pyplot.ylim(*ylim)
        pyplot.hlines(0, *xlim, colors = 'r', linestyles = 'dashed')
        pyplot.xlabel(
            'Soil %s' % 'Moisture (%)' if driver == 'soil_m' else 'Temperature (K)')
        pyplot.ylabel('Observed minus Predicted RH')
        pyplot.title('PFT=%d' % pft)
        pyplot.show()

    def print(self, pft, **kwargs):
        '''
        Prints current parameter values for the given PFT.

        Parameters
        ----------
        pft : int
            The chosen PFT class
        '''
        try:
            with open(STORAGE_PATH, 'rb') as file:
                data_dict = pickle.load(file)
        except:
            raise ValueError('No fit data for the specified PFT')
        labels = data_dict['metadata']
        self._report_params(
            labels, data_dict['parameters'][(pft-1),:], [np.nan] * len(labels))

    def print_all(self, parameter):
        '''
        Prints the parameter value across all PFTs.

        Parameters
        ----------
        parameter : str
            Name of the model parameter
        '''
        try:
            with open(STORAGE_PATH, 'rb') as file:
                data_dict = pickle.load(file)
        except:
            raise ValueError('No fit data for the specified PFT')
        assert parameter in data_dict['metadata'],\
            'Not a recognized parameter name for this model'
        p = data_dict['metadata'].index(parameter)
        labels = []
        values = []
        for i in range(0, data_dict['parameters'].shape[0]):
            labels.append('PFT%d' % (i+1))
            values.append(data_dict['parameters'][i,p])
        print('Fit values for parameter "%s"' % parameter)
        self._report_params(labels, values, [np.nan] * len(values))

    def run(self, pft, **kwargs):
        '''
        Main entry point for L4C-DAMM model calibration. Additional keyword
        arguments (**kwargs) passed to the optimizer. There are two general search
        modes: an initial, "global" search of the parameter space and a follow-up
        "local" search of a smaller parameter space centered on the current local
        optimum. The local search is deterministic; if it converges, it will
        always converge on the local optimum. The global search is not
        deterministic and, as the presence of multiple local optima is always
        a possibility, the global search should be run in multiple "trials," where
        each trial uses a different, randomized set of initial parameter values.
        Important considerations:
            1) Tower GPP and RECO values <0 are masked;
            2) Tower GPP is masked where APAR is <0.1 MJ m-2 d-1;

        The free parameters to be calibrated are:
            alpha   Pre-exponential factor of enzymatic reaction with S_x
                    (Mg C cm-3 hr-1) # NOTE: Megagrams of C...
            ea      Activation energy of enzymatic reaction with S_x (kJ mol-1)
            km_s    Michaelis-Menten (MM) coefficient for subtrate, using the
                    constant-value form (g C cm-3)
            km_s*   NOTE: In DAMMDecompositionModel2, this is the slope term of
                    the temperature-sensitive MM coef. for substrate and the
                    previous term is the intercept; in DAMMDecompositionModel
                    this term does not exist.
            p       Proportion of C_total that is soluble
            d_liq   Diffusion coefficient of substrate in liquid phase
            d_gas   Diffusion coefficient of O_2 in air
            km_O2   Half-saturation MM coefficient for O_2 in air
            cue     Carbon-use efficiency (CUE), fit ONLY if --fit-cue=True

        Considerations:
            1) Negative RH values (i.e., NPP > RECO) are set to zero.
            2) Estimating the initial soil C pool size using the steady-state
                equations (invert_fluxes = False) leads to an understimate
                of soil C pools; better to use the empirical C storage
                estimate ("Cbar") that comes from inverting the RH fluxes.
            3) If --fit-cue=True, then tower RECO is used for fitting
                instead of tower RH.
        '''
        self._load_data(pft)
        gpp_tower, reco_tower, soil_m, soil_t, porosity, litterfall, weights = [
            self._data[k] for k in list(self._data.keys())[0:7]
        ]
        # Choose initial values, bounds, etc. based on the model and search
        init_params, bounds, step_size, labels = self._configure(pft)
        # Verify that fixed parameters are named properly
        if len(kwargs) > 0:
            for param, param_value in kwargs.items():
                try:
                    p = labels.index(param)
                except ValueError:
                    raise ValueError('Parameter "%s" not found' % param)
        if self._trials == 0:
            # Don't fit the model, just report the goodness-of-fit
            try:
                with open(STORAGE_PATH, 'rb') as file:
                    data_dict = pickle.load(file)
                    results = data_dict['parameters']
            except:
                raise ValueError('No fit data for the specified PFT')
            init_params = results[pft-1,:]
            fit_params = init_params
            # Optionally set certain parameter values manually
            if len(kwargs) > 0:
                for param, param_value in kwargs.items():
                    fit_params[labels.index(param)] = param_value
            assert not np.all(np.isnan(init_params))
            self._report_fit(init_params, fit_params, labels)
            return

        # Get a residuals function that already knows about the data
        residuals = partial(
            self._residuals, **self._data, mlow = self._moisture_ptile0,
            mhigh = self._moisture_ptile1, tlow = self._temp_ptile,
            labels = labels)
        scores = [np.nan] * self._trials
        params = []
        param_space = np.linspace(bounds[0], bounds[1], 100)
        for i in range(0, self._trials):
            # Randomize the initial parameter values and score each model
            if self._trials > 1:
                p = param_space.shape[1] # Number of parameters
                idx = np.random.randint(0, param_space.shape[0], p)
                init_params = param_space[idx,np.arange(0, p)]
            # Optionally set certain parameter values manually
            if len(kwargs) > 0:
                for param, param_value in kwargs.items():
                    init_params[labels.index(param)] = param_value
            # Run the optimization!
            if self._method == 'nlopt':
                opt = GenericOptimization(
                    residuals, bounds, step_size = step_size,
                    method = nlopt.GN_ISRES if self._search == 'global' else nlopt.LN_COBYLA)
                fit_params = opt.solve(init_params[0:len(labels)])
            elif self._method == 'scipy':
                solution = solve_least_squares(
                    residuals, init_params, labels, bounds, jac = '3-point')
                fit_params = solution.x.tolist()
            # One of many possible parameter sets...
            params.append(fit_params)
            # ...To be chosen on the basis of its goodness-of-fit score
            r_squared, rmse_score, _, _ = self._report_fit(
                init_params, fit_params, labels, plots = False)
            scores[i] = rmse_score if self._minimize_rmse else r_squared

        best = params[np.argmin(scores)] if self._minimize_rmse else params[np.argmax(scores)]
        self.update_store(pft, best, labels)

    @classmethod
    def sites(cls, pft, excluded = [], legacy = True):
        '''
        For a given PFT class, returns the tower sites, as rank indices, that
        represent that PFT. Exceptions are made according to the L4C
        calibration protocol, e.g., sites with any amount of
        Deciduous Needleleaf (DNF) in their 1-km subgrid are considered to
        represent the DNF PFT class.

        Parameters
        ----------
        pft : int
        excluded : list or tuple

        Returns
        -------
        numpy.ndarray
        '''
        with h5py.File(INPUTS_HDF, 'r') as hdf:
            nsites, _ = hdf['state/PFT'].shape
            if legacy:
                pft_map = hdf['legacy/lc_dom'][:].swapaxes(0, 1)
            else:
                pft_map = hdf['state/PFT'][:]
            if len(excluded) > 0:
                site_ids = hdf['site_id'][:]

        idx = np.arange(0, nsites)
        if pft == 3:
            return np.apply_along_axis(
                lambda x: x == 3, 1, pft_map).any(axis = 1)

        # Return boolean index for sites that match PFT and *not* in excluded
        if len(excluded) > 0:
            return np.logical_and(
                np.equal(pft, pft_dominant(pft_map)),
                ~np.in1d(site_ids, excluded))
        return np.equal(pft, pft_dominant(pft_map))

    def update_store(self, pft, fit_params, labels):
        '''
        Updates the store of fitted model parameters.

        Parameters
        ----------
        pft : int
        fit_params : list or tuple or numpy.ndarray
        labels : list or tuple
            Parameter names in order
        '''
        if os.path.exists(STORAGE_PATH):
            try:
                with open(STORAGE_PATH, 'rb') as file:
                    data_dict = pickle.load(file)
                    results = data_dict['parameters']
            except EOFError:
                pass # Probably just an empty file
        else:
            # 8 PFTs, N fit parameters
            results = np.ones((8, len(fit_params))) * np.nan
        # Only update the parameters if user says so, unless this is the first
        #   time fit parameters are available
        if np.isnan(results[pft-1,:]).all():
            do_write = True
        else:
            user_prompt = input('Update parameters for PFT=%d? [Y/n] ' % pft)
            do_write = user_prompt == 'Y'
        if do_write:
            print('Updating parameters for PFT=%d...' % pft)
            # Reshape array if we've added parameters
            if results.shape[1] < len(fit_params):
                d = abs(results.shape[1] - len(fit_params))
                results = np.hstack((results, np.ones((results.shape[0], d)) * np.nan))
            results[pft-1,0:len(fit_params)] = np.array(fit_params)
            # Also truncate the results if we have *fewer* parameters than we used to
            if results.shape[1] > len(fit_params):
                results[pft-1,len(fit_params):] = np.nan
            with open(STORAGE_PATH, 'wb') as file:
                pickle.dump({
                    'metadata': labels,
                    'parameters': results
                }, file)


def smooth_series(arr, w):
    '''
    Apply a zero-phase offset moving average FIR filter to smooth time-series
    data.

    Parameters
    ----------
    arr : numpy.ndarray
    w : int

    Returns
    -------
    numpy.ndarray
    '''
    def ffill(arr):
        '''
        Forward-filling of NaNs in the direction of the second axis (axis = 1).
        https://stackoverflow.com/questions/41190852/
            most-efficient-way-to-forward-fill-nan-values-in-numpy-array
        '''
        mask = np.isnan(arr)
        idx = np.where(~mask, np.arange(mask.shape[1]), 0)
        np.maximum.accumulate(idx, axis=1, out=idx)
        out = arr[np.arange(idx.shape[0])[:,None], idx]
        return out

    def bfill(arr):
        '''
        Backward-filling of NaNs in the (reverse) direction of the second axis
        (axis = 1).
        https://stackoverflow.com/questions/41190852/
            most-efficient-way-to-forward-fill-nan-values-in-numpy-array
        '''
        mask = np.isnan(arr)
        idx = np.where(~mask, np.arange(mask.shape[1]), mask.shape[1] - 1)
        idx = np.minimum.accumulate(idx[:, ::-1], axis=1)[:, ::-1]
        out = arr[np.arange(idx.shape[0])[:,None], idx]
        return out

    mask = np.isnan(arr)
    arr = np.apply_along_axis(
        lambda x: filtfilt(np.ones((w,)) / w, (1,), x, method = 'gust')
            if not np.isnan(x).all() else x, 0, ffill(bfill(arr.T)).T)
    arr[mask] = np.nan
    return arr



if __name__ == '__main__':
    import fire
    fire.Fire(CLI)
